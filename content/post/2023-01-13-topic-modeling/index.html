---
title: "Identifying Salient Issues in Interstate Conflicts Using Topic Modeling"
author: "Sebastian Cujai"
date: '2023-01-13'
slug: topic-modeling
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: ''
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
output: 
  html_document:
    toc: true
bibliography: references.bib
link-citations: yes
csl: apa-single-spaced.csl
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>In this post, I am looking for more detailed insights regarding the causes of conflict escalation in Georgian-Russian relations between 2000 and 2013.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> By looking at the intensity trends of conflict actions (Fig. 1), we observe two peaks in the intensity trend of the Russian side (2004, 2008) and two peaks in the intensity trend of the Georgian side (2006, 2008).</p>
<p><strong><em>Fig. 1</em></strong> The intensity trends of Russian and Georgian conflict actions
<img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-2-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>To gain further insights on these conflict escalations, I search for salient issues in Georgian-Russian relations within news articles from the New York Times by using Topic Modeling. This unsupervised machine learning technique aims to find topics or clusters inside a text corpus without any external dictionaries or training data. In this post, I will use the most popular approach for Topic Modeling, the Latent Dirichlet Allocation (LDA) <span class="citation">(<a href="#ref-blei_latent_2003" role="doc-biblioref">Blei et al., 2003</a>)</span>. This post builds heavily on different tutorials on text mining using R <span class="citation">(<a href="#ref-niekler_hands-_2017" role="doc-biblioref">Niekler &amp; Wiedemann, 2017</a>; <a href="#ref-schweinberger_topic_2022" role="doc-biblioref">Schweinberger, 2022</a>)</span>.</p>
<p><br></p>
<div id="data-preparation" class="section level3">
<h3>Data preparation</h3>
<p>The process starts with the reading of the text data. The texts’ length affects the results of topic modeling. Therefore, for very short texts (e.g., Twitter posts) or very long texts (e.g., books), it is advisable to concatenate/split single documents to get longer/shorter text units. By performing a qualitative review of the results, we can determine if these approaches leads to more interpretable topics. After a brief review of my results, I see no reason to use these approaches for analyzing the articles at hand.</p>
<p>For text preprocessing, I stem words and convert letters to lowercase. I also remove special characters and stop words (i.e., function words that have relational rather than substantive meaning). Stop words are problematic because they appear as “noise” in the estimated topics generated by the LDA model.</p>
<p>Thereafter, I create a Document-Term-Matrix (DTM) of the corpus. This matrix describes the frequency of terms that occur in a text collection, where the rows correspond to texts in the collection and the columns correspond to terms. Thereby, I only consider terms that occur with a minimum frequency of 15 times within the corpus. This is primarily to speed up the model calculation.</p>
<p><br></p>
</div>
<div id="calculate-topic-model" class="section level3">
<h3>Calculate topic model</h3>
<p>The calculation of topic models aims to determine the proportional composition of a fixed number of topics in the documents of a collection. For parameterized models such as LDA, the <strong><em>number of topics (K)</em></strong> is the most important parameter to define in advance. It is worth to experiment with different parameters to find the most suitable parameters for your own analytical needs. If K is too small, the collection will be split into a few very general topics. If K is too large, the collection will be divided into far too many topics, where some overlap and others are almost impossible to interpret.</p>
<p>One procedure for deciding on a specific topic number is the usage of the ldatuning package which uses some metrics to find optimal number of topics for LDA models <span class="citation">(<a href="#ref-nikita_ldatuning_2020" role="doc-biblioref">Nikita &amp; Chaney, 2020</a>)</span>. This approach can be useful when the number of topics is not theoretically motivated or based on a qualitative data inspection. For illustrative purposes, I use the two methods CaoJuan2009 and Griffith2004. But it is recommendable to inspect the results of the four metrics available (Griffiths2004, CaoJuan2009, Arun2010, and Deveaud2014). Furthermore, I choose a thematic resolution of 20 topics. In contrast to a resolution of 100 or more, this number of topics can be easily evaluated qualitatively.
We can now plot the results. The best number of topics shows high values for Griffith2004 and low values for CaoJuan2009. Optimally, several methods should converge and show peaks and drops respectively for a certain number of topics. The inference of topic models can take a long time depending on the vocabulary size, the collection size, and K. This calculation takes several minutes. If it takes too long, we can reduce the vocabulary in the DTM by increasing the minimum frequency. Looking at the graphical representation (Fig. 2), we can conclude that the optimal number of topics is 16. We use this number to compute the LDA model.</p>
<p><strong><em>Fig. 2</em></strong> Finding the optimal number of topics</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The model calculation results in two probability distributions. The first result, called theta here, shows the distribution of topics within each document. The second outcome, named beta, shows the probability that a word appears in a specific topic.</p>
<p>Depending on our research goals, we might be interested in a more pointed or a more even distribution of topics in the model. The <strong><em>distribution of topics</em></strong> within a document can be controlled with the alpha parameter of the model. In the first model calculation the alpha parameter was automatically estimated in order to fit the data (highest overall probability of the model). If we now increase the alpha value, we get a more even distribution of topics within a document. If we decrease the alpha value, the inference process distributes the probability mass on a few topics for each document.</p>
<p>In the following case, I am interested in the most salient topics within the interstate relations. Therefore, I decide for a more pointed distribution of topics and change the alpha parameter to a lower value to create a second model. To see how this affects the distribution, I compare the topic distribution within three sample documents according to both models. For the next steps, we give the topics more descriptive names by concatenating the five most likely terms of each topic to a string that represents a pseudo-name for each topic. In the first model, all three documents show at least a small percentage of each topic. In the second model, all three documents only show a few dominant topics (Fig. 3).</p>
<p><strong><em>Fig. 3</em></strong> Effect of a modified alpha parameter on topic distribution</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-8-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="overview-of-topic-related-terms" class="section level3">
<h3>Overview of topic related terms</h3>
<p>Relying on the results of the second model, I create a word cloud for each topic which gives a quick visual overview on the related terms. For the sake of clarity, I only select the 40 terms with the highest probability to appear in a given topic (Fig. 4). At first glance, relying on my case knowledge, a large number of the identified topics appear to be valid. But some topics seem either too general (e.g., peopl_year_day_time_live) or entirely irrelevant (e.g., iraq_weapon_nuclear_offici_american).</p>
<p><strong><em>Fig. 4</em></strong> Illustration of topic-related terms</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="topics-over-time" class="section level3">
<h3>Topics over time</h3>
<p>In the next step, I provide an overview of the occurrence of topics within the articles over time. For this purpose, I determine the average share of topics in the annual news coverage and visualize these aggregated topic proportions using a bar plot (Fig. 5). I can now use this bar plot to identify topics related to interstate conflict events.</p>
<p><strong><em>Fig. 5</em></strong> Topics over time</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>By comparing the intensity trends of conflict actions with the aggregated topic proportions, we see that topics around Russian energy supplies as well as legal procedures clearly dominate the year 2006. The breakaway regions South Ossetia and Abkhazia as well as the Georgian NATO membership are the most important topics in the year 2008. However, some topics are very nebulous and require further background information.</p>
<p>In summary, we can conclude that topic modeling is a very helpful approach for detecting salient issues in interstate conflicts from scratch. We can use these results to look at the occurrence of topics over time. Thereby, we should bear in mind that the representation of topics depends on the composition of the underlying text collection.</p>
<p>Furthermore, we can use the topic model for thematic filtering of a text collection based on the topic probabilities for each document. By investigating these thematic text collections, we can use information retrieval approaches (e.g., the implicit network approach) to identify relations between these topics and other potential entities (e.g., state leaders, dates, and locations) and to dig deeper into the context (see <span class="citation">Cujai (<a href="#ref-mello_big_2022" role="doc-biblioref">2022</a>)</span> for further details).</p>
<p><br></p>
</div>
<div id="references" class="section level3 unnumbered">
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-blei_latent_2003" class="csl-entry">
Blei, D. M., Jordan, M. I., &amp; Ng, A. Y. (2003). Latent <span>Dirichlet</span> <span>Allocation</span>. <em>Journal of Machine Learning Research</em>, <em>3</em>, 993–1022.
</div>
<div id="ref-mello_big_2022" class="csl-entry">
Cujai, S. (2022). Big <span>Data</span> <span>Analysis</span>. In P. A. Mello &amp; F. Ostermann (Eds.), <em>Routledge <span>Handbook</span> of <span>Foreign</span> <span>Policy</span> <span>Analysis</span> <span>Methods</span></em> (First Edition, pp. 485–501). Routledge.
</div>
<div id="ref-niekler_hands-_2017" class="csl-entry">
Niekler, A., &amp; Wiedemann, G. (2017, September). <em>Hands-on: <span>A</span> <span>Five</span> <span>Day</span> <span>Text</span> <span>Mining</span> <span>Course</span> for <span>Humanists</span> and <span>Social</span> <span>Scientists</span> in <span>R</span></em>. <a href="https://tinyurl.com/2xmd69aa">https://tinyurl.com/2xmd69aa</a>
</div>
<div id="ref-nikita_ldatuning_2020" class="csl-entry">
Nikita, M., &amp; Chaney, N. (2020). <em>Ldatuning: <span>Tuning</span> of the <span>Latent</span> <span>Dirichlet</span> <span>Allocation</span> <span>Models</span> <span>Parameters</span></em>. <a href="https://CRAN.R-project.org/package=ldatuning">https://CRAN.R-project.org/package=ldatuning</a>
</div>
<div id="ref-schweinberger_topic_2022" class="csl-entry">
Schweinberger, M. (2022). Topic <span>Modeling</span> with <span>R</span>. In <em>Language Technology and Data Analysis Laboratory (LADAL)</em>. <a href="https://ladal.edu.au/topicmodels.html#ref-silge2017text">https://ladal.edu.au/topicmodels.html#ref-silge2017text</a>
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>The presented conflict trends are drawn from an adjusted version of the ICEWS data set which only contains interstate conflict actions. The data selection process reveals limited availability of data on interstate conflict events explaining the shortened investigation period. For more information on the selection process, see Cujai, S. (2021): Determination of Interstate Conflict Trends with ICEWS. <a href="https://tinyurl.com/2dyh67su" class="uri">https://tinyurl.com/2dyh67su</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
