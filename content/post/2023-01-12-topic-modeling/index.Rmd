---
title: "Identifying Salient Issues in Interstate Conflicts Using Topic Modeling"
author: "Sebastian Cujai"
date: '2023-01-12'
slug: topic-modeling
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: ''
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
output: 
  html_document:
    toc: true
bibliography: references.bib
link-citations: yes
csl: apa-single-spaced.csl
---

```{r, include=FALSE}
# https://www.youtube.com/watch?v=vLzXDzf8bNI 
library(knitr)
# define options
show = FALSE
opts_chunk$set(echo = show,
               error = show,
               warning = show,
               message = show)
Sys.setlocale("LC_ALL", "English")
```

<style type="text/css">
div#TOC li {
    list-style:none;}
div#TOC ul {
    padding-left:0;}
div {
    line-height: 2em;}
</style>

### Introduction

In this post, I am looking for more detailed insights regarding the causes of conflict escalation in Georgian-Russian relations between 2000 and 2013.[^1] By looking at the intensity trends of conflict actions (Fig. 1), we observe two peaks in the intensity trend of the Russian side (2004, 2008) and two peaks in the intensity trend of the Georgian side (2006, 2008). 

***Fig. 1*** The intensity trends of Russian and Georgian conflict actions
```{r, fig.height = 5, fig.width = 8, fig.align = 'center'}
library(tidyverse)
library(here)
library(tm)
library(ldatuning)
library(topicmodels)
library(reshape2)
library(pals)
library(wordcloud)
library(gridExtra)
library(data.table)
library(xml2)
library(icews)
library(lubridate)
library(scales)
#--------------------
# determinate interstate conflict trends with ICEWS
#--------------------
# set working directory
setwd(here("content","post","2023-01-12-topic-modeling","ICEWSdata"))
# load event data files (1995-2019)
eventFiles = list.files(pattern = "*.rds") %>%
  map_df(~readRDS(.)) 
# add new variables
.addNewCameo = function(x) {
  cameo_codes %>%
    mutate(new_col = stringr::str_replace_all(cameo_code, "^0","")) %>%
    rename(CAMEO.Code = new_col) %>%
    filter(!is.na(lvl1)) %>%
    select(CAMEO.Code,name,quad_category,penta_category, order, lvl0) %>%
    mutate_at(vars(CAMEO.Code),list(as.integer)) %>% # https://stackoverflow.com/questions/27668266/dplyr-change-many-data-types
    right_join(x,by = "CAMEO.Code")
}
eveICEWS = .addNewCameo(eventFiles)
# change country names// !!! Different names for countries
.changeNames = function(x,y,z){
  df = z %>% 
    mutate(Source.Country = str_replace_all(Source.Country,x,y)) %>%
    mutate(Target.Country = str_replace_all(Target.Country,x,y))
  return(df)
}
eventsICEWS = .changeNames("Russian Federation","Russia",eveICEWS) %>% 
  select(Event.Date,Source.Country, Source.Name, Target.Country, Target.Name, 
         CAMEO.Code, lvl0, quad_category)
# delete files from global environment
rm("eventFiles","eveICEWS")
# set new working directory
setwd(here("content","post","2023-01-12-topic-modeling"))
# choose conflict dyad 
sideA = "Georgia"
sideB = "Russia"
# create data set (country level)
.selectCountrylevel = function (ff,gg) {
  selectionA = function(x) {
    x %>%
      mutate(new_col = stringr::str_replace_all(Event.Date, "-[0-9]{2}$" , "-01")) %>%
      filter(str_detect(Source.Country, ff) & str_detect(Target.Country, gg)) %>%
      dplyr::select (new_col, Source.Country, Source.Name, Target.Country, Target.Name, CAMEO.Code, lvl0, quad_category) %>%
      rename(Event.Date = new_col) %>%
      filter(str_detect(quad_category, "material conflict|verbal conflict")) 
  }
  selectionB = function(x) {
    x %>%
      mutate(new_col = stringr::str_replace_all(Event.Date, "-[0-9]{2}$" , "-01")) %>%
      filter(str_detect(Source.Country, gg) & str_detect(Target.Country, ff)) %>%
      dplyr::select (new_col, Source.Country, Source.Name, Target.Country, Target.Name, CAMEO.Code, lvl0, quad_category) %>%
      rename(Event.Date = new_col) %>%
      filter(str_detect(quad_category, "material conflict|verbal conflict"))
  }
  dyad = bind_rows(selectionA (eventsICEWS),selectionB (eventsICEWS))
  return(dyad)
}
dataCountryLevel = .selectCountrylevel(sideA,sideB)
# create data set (actor level, narrow) 
.selectActorLevelN = function (zz,ff,gg) {
  # identify all active conflict actors
  .selectInstPer = function(x) {
    df = x %>%
      select(Source.Name) %>%
      distinct() %>%
      as_tibble()
    return(df)
  }
  InstPer = .selectInstPer(dataCountryLevel)
  # extract only state institutions
  .selectInst = function(x) {
    # extract the most relevant state institutions // https://stackoverflow.com/questions/23678691/converting-a-text-file-into-a-vector-in-r
    relStateInst = scan(file = "relevant_institutions.txt", character(), quote = "", sep = ",")
    instList = paste(unlist(relStateInst), collapse = "|")
    stateInst = x %>%
      filter (str_detect(Source.Name, instList))
    return(stateInst) }
  inst = .selectInst(InstPer)
  # delete wrong results
  .deleteAct = function(x) {
    deleteSet = scan(file = "irrelevant_institutions.txt", character(), quote = "", sep = ",")
    delInst = paste(unlist(deleteSet), collapse = "|")
    finListInst = x %>%
      filter (!str_detect(Source.Name, delInst))
    return(finListInst)}
  finInst = .deleteAct(inst)
  # create a list of all leaders and government officials till 2009
  .pullLeaders = function(x) {
    df = read_xml("leader_FP.xml")
    data = tibble(
      names = df %>% xml_find_all(paste0(".//",x,"//.//.//name")) %>% xml_text()) %>%  
      distinct()
    return(data)
  }
  finPer = bind_rows(.pullLeaders(sideA),.pullLeaders(sideB))
  # compare the list of state actors with all conflict actors => only state actors
  .chooseActivePer = function(x,y) {
    actList = paste(unlist(x$names), collapse = "|")
    stateAct = y %>%
      filter (str_detect(Source.Name, actList))
    return(stateAct)
  }
  finPer = .chooseActivePer (finPer,InstPer)
  # create a list of all active state actors
  finInstPer = bind_rows(finInst,finPer)
  # create new data set based on state leaders and institutions
  .chooseStateEvents = function(x) {
    compiNames = as.vector(finInstPer$Source.Name)
    sourceList = x %>%
      filter(Source.Name %in% compiNames) %>%
      filter(Target.Name %in% compiNames) 
    return(sourceList)
  }
  confEvents = .chooseStateEvents(zz)
  return(confEvents)
}
dataActorLevelN = .selectActorLevelN(dataCountryLevel,sideA,sideB)
# plot conflict intensity trend line (actor level, narrow) 
.monConIntTrend = function(df,ff,gg,tStart,tEnd,smoothVal) {
  .monConInt = function(x) {
    df %>%
      filter(Source.Country == x) %>%
      select(Event.Date,lvl0) %>%
      filter(Event.Date >= tStart & Event.Date <= tEnd) %>%
      group_by(Event.Date) %>%
      slice(which.max(lvl0)) %>%
      rename(intensity = lvl0) %>%
      mutate(name = rep(x)) %>%
      as.data.frame(stringsAsFactors = FALSE)
  }
  monConIntAB = bind_rows (monConIntA = .monConInt(ff), monConIntB = .monConInt(gg)) %>%
    # changing datetime format for ggplot
    mutate_at(vars(Event.Date),as.POSIXct) %>%
    # Rename and re-order the factor levels before the plot 
    mutate(name = factor(name,levels = c(sideA,sideB)))
  plot = monConIntAB %>% 
    ggplot(aes(x = Event.Date, y = intensity, group = name, color = name)) +
    geom_point()+
    geom_smooth(method = "loess", span = smoothVal, se = FALSE) +
    labs (caption = "Source: ICEWS 2000-2012",
          x = "dates",
          y = "intensity",
          color = "") +
    scale_x_datetime(labels = date_format("%b %y"), 
                     breaks = date_breaks("6 month")) +
    scale_y_continuous(breaks = seq(8,21, by = 1)) +
    theme_light() +
    theme(axis.text.x = element_text(angle = 60, vjust = 1, size = 8, margin = margin (13,13,0,0)),
          plot.margin = unit(c(0, 2, 1, 2), "cm"),
          legend.position = "top")
  return(plot)}
.monConIntTrend(dataActorLevelN,sideA,sideB,tStart = "2000",tEnd = "2013",smoothVal = 0.4)

```

To gain further insights on these conflict escalations, I search for salient issues in Georgian-Russian relations within news articles from the New York Times by using Topic Modeling. This unsupervised machine learning technique aims to find topics or clusters inside a text corpus without any external dictionaries or training data. In this post, I will use the most popular approach for Topic Modeling, the Latent Dirichlet Allocation (LDA) [@blei_latent_2003]. This post builds heavily on different tutorials on text mining using R [@schweinberger_topic_2022;@niekler_hands-_2017].

<br>

### Data preparation

The process starts with the reading of the text data. The textsâ€™ length affects the results of topic modeling. Therefore, for very short texts (e.g., Twitter posts) or very long texts (e.g., books), it is advisable to concatenate/split single documents to get longer/shorter text units. By performing a qualitative review of the results, we can determine if these approaches leads to more interpretable topics. After a brief review of my results, I see no reason to use these approaches for analyzing the articles at hand.

For text preprocessing, I stem words and convert letters to lowercase. I also remove special characters and stop words (i.e., function words that have relational rather than substantive meaning). Stop words are problematic because they appear as "noise" in the estimated topics generated by the LDA model.

Thereafter, I create a Document-Term-Matrix (DTM) of the corpus. This matrix describes the frequency of terms that occur in a text collection, where the rows correspond to texts in the collection and the columns correspond to terms. Thereby, I only consider terms that occur with a minimum frequency of 15 times within the corpus. This is primarily to speed up the model calculation. 

<br>

### Calculate topic model

The calculation of topic models aims to determine the proportional composition of a fixed number of topics in the documents of a collection. For parameterized models such as LDA, the ***number of topics (K)*** is the most important parameter to define in advance. It is worth to experiment with different parameters to find the most suitable parameters for your own analytical needs. If K is too small, the collection will be split into a few very general topics. If K is too large, the collection will be divided into far too many topics, where some overlap and others are almost impossible to interpret.

One procedure for deciding on a specific topic number is the usage of the ldatuning package which uses some metrics to find optimal number of topics for LDA models [@nikita_ldatuning_2020]. This approach can be useful when the number of topics is not theoretically motivated or based on a qualitative data inspection. For illustrative purposes, I use the two methods CaoJuan2009 and Griffith2004. But it is recommendable to inspect the results of the four metrics available (Griffiths2004, CaoJuan2009, Arun2010, and Deveaud2014). Furthermore, I choose a thematic resolution of 20 topics. In contrast to a resolution of 100 or more, this number of topics can be easily evaluated qualitatively. 
We can now plot the results. The best number of topics shows high values for Griffith2004 and low values for CaoJuan2009. Optimally, several methods should converge and show peaks and drops respectively for a certain number of topics. The inference of topic models can take a long time depending on the vocabulary size, the collection size, and K. This calculation takes several minutes. If it takes too long, we can reduce the vocabulary in the DTM by increasing the minimum frequency. Looking at the graphical representation (Fig. 2), we can conclude that the optimal number of topics is 16. We use this number to compute the LDA model. 

```{r, include=FALSE}
#----------------------
# load data and create corpus
#---------------------
setwd(here("content","post","2023-01-12-topic-modeling"))
textdata = as.data.frame(readRDS("NYT_Georgia+Russia_processed.rds")) %>% 
  rename(doc_id = ID) %>% 
  filter(date >= 2000 & date <= 2014) 
english_stopwords = readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus = Corpus(DataframeSource(textdata))
#-----------------------
# preprocess text data
#----------------------
processedCorpus = tm_map(corpus, content_transformer(tolower))
processedCorpus = tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus = tm_map(processedCorpus, str_remove_all, "([_])|[[:punct:]]")
processedCorpus = tm_map(processedCorpus, removeNumbers)
processedCorpus = tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus = tm_map(processedCorpus, stripWhitespace)
# consider terms that occur with a certain minimum frequency in the body
dtm = DocumentTermMatrix(processedCorpus, control=list(bounds = list(global = c(15,Inf))))
# due to vocabulary pruning, we have empty rows in our DTM LDA does not like this. 
# So we remove those docs from the DTM and the metadata
sel_idx = slam::row_sums(dtm) > 0
dtm = dtm[sel_idx, ]
textdata = textdata[sel_idx, ]
#----------------------
# find the best number of topics
#----------------------
# create models with different number of topics
result = FindTopicsNumber(dtm,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE)

```

***Fig. 2*** Finding the optimal number of topics 

```{r, fig.height = 5, fig.width = 7, fig.align = 'center'}
# the best number of topics shows low values for CaoJuan2009 and high values for Griffith2004 
FindTopicsNumber_plot(result)
```

The model calculation results in two probability distributions. The first result, called theta here, shows the distribution of topics within each document. The second outcome, named beta, shows the probability that a word appears in a specific topic. 

```{r, include = FALSE}
# number of topics
K = 16
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel = LDA(dtm, K, method = "Gibbs", control = list(iter = 500, verbose = 25))
# have a look a some of the results (posterior distributions)
tmResult = posterior(topicModel)
# topics are probability distributions over the entire vocabulary
beta = tmResult$terms
# for every document we have a probability distribution of its contained topics
theta = tmResult$topics
```

Depending on our research goals, we might be interested in a more pointed or a more even distribution of topics in the model. The ***distribution of topics*** within a document can be controlled with the alpha parameter of the model. In the first model calculation the alpha parameter was automatically estimated in order to fit the data (highest overall probability of the model). If we now increase the alpha value, we get a more even distribution of topics within a document. If we decrease the alpha value, the inference process distributes the probability mass on a few topics for each document. 

In the following case, I am interested in the most salient topics within the interstate relations. Therefore, I decide for a more pointed distribution of topics and change the alpha parameter to a lower value to create a second model. To see how this affects the distribution, I compare the topic distribution within three sample documents according to both models. For the next steps, we give the topics more descriptive names by concatenating the five most likely terms of each topic to a string that represents a pseudo-name for each topic. In the first model, all three documents show at least a small percentage of each topic. In the second model, all three documents only show a few dominant topics (Fig. 3). 

***Fig. 3*** Effect of a modified alpha parameter on topic distribution

```{r,fig.height = 6,fig.width = 8,fig.align = 'center'}
# create pseudo-name for each topic by concatenating the five most likely terms of each topic to a string 
top5termsPerTopic = terms(topicModel, 5)
topicNames = apply(top5termsPerTopic, 2, paste, collapse="_")
# look at the contents of three sample documents
exampleIds = c(100, 500, 1000)
# visualize the topic distributions within the documents
N = length(exampleIds)
# get topic proportions form example documents
topicProportionExamples = theta[exampleIds,]
colnames(topicProportionExamples) = topicNames
vizDataFrame = melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")
first = ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") + 
  geom_bar(stat = "identity") +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_fill_manual(values = kelly(17)[5:7], name = "Documents") +
  coord_flip() +
  facet_wrap(~ document, ncol = N)
```
```{r, include=FALSE}
# The topic distribution within a document can be controlled with the Alpha-parameter of the model.
# attr(topicModel, "alpha") # see alpha from previous model
topicModel2 = LDA(dtm, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 2))
tmResult = posterior(topicModel2)
theta = tmResult$topics
beta = tmResult$terms
# terms(topicModel2, 10)
topicNames = apply(terms(topicModel2, 5), 2, paste, collapse = "_") 
```
```{r,fig.height = 6,fig.width = 8,fig.align = 'center'}
# get topic proportions form example documents
topicProportionExamples = theta[exampleIds,]
colnames(topicProportionExamples) = topicNames
vizDataFrame = melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")
sec = ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_fill_manual(values = kelly(17)[5:7], name = "Documents") +
  coord_flip() +
  facet_wrap(~ document, ncol = N)
grid.arrange(first,sec, nrow = 2, heights = c(10,10))
```

<br>

### Overview of topic related terms

Relying on the results of the second model, I create a word cloud for each topic which gives a quick visual overview on the related terms. For the sake of clarity, I only select the 40 terms with the highest probability to appear in a given topic (Fig. 4). At first glance, relying on my case knowledge, a large number of the identified topics appear to be valid. But some topics seem either too general (e.g., peopl_year_day_time_live) or entirely irrelevant (e.g., iraq_weapon_nuclear_offici_american).  

***Fig. 4*** Illustration of topic-related terms 

```{r,fig.height = 6,fig.width = 8,fig.align = 'center'}
my_colours = kelly(17)[1:17]
gradient_base = my_colours[1]
my_gradients = map(my_colours[2:17], function(x) colorRampPalette(c(gradient_base,x))(5))
par(mfrow = c(4, 4), mar = c(0.5, 0.5, 0.5,0.5))
loop.vector = 1:16
for (i in loop.vector) {
  top40terms = sort(tmResult$terms[i,], decreasing = TRUE)[1:40]
  wordcloud(names(top40terms),as.numeric(top40terms), 
             scale=c(3,.8), random.order = F, random.color = T,
            colors= my_gradients[[i]][3:4])}
```

<br>

### Topics over time

In the next step, I provide an overview of the occurrence of topics within the articles over time. For this purpose, I determine the average share of topics in the annual news coverage and visualize these aggregated topic proportions using a bar plot (Fig. 5). I can now use this bar plot to identify topics related to interstate conflict events. 

***Fig. 5*** Topics over time 

```{r, fig.height = 5, fig.width = 8, fig.align = 'center'}
text = textdata %>% mutate(dates = str_replace_all(date,"-[0-9]{2}-[0-9]{2}$",""))
# get mean topic proportions per decade
topic_proportion_per_decade = aggregate(theta, by = list(dates = text$dates), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] = topicNames
# reshape data frame
vizDataFrame = melt(topic_proportion_per_decade, id.vars = "dates")
# plot topic proportions per decade as bar plot
ggplot(vizDataFrame, aes(x = dates, y = value, fill = variable)) + 
  geom_bar(stat = "identity") +
  theme_light() +
  ylab("proportion") + 
  scale_fill_manual(values = kelly(17)[2:17], name = "topics") + 
  theme(axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 0.8))
```

By comparing the intensity trends of conflict actions with the aggregated topic proportions, we see that topics around Russian energy supplies as well as legal procedures clearly dominate the year 2006. The breakaway regions South Ossetia and Abkhazia as well as the Georgian NATO membership are the most important topics in the year 2008. However, some topics are very nebulous and require further background information.

In summary, we can conclude that topic modeling is a very helpful approach for detecting salient issues in interstate conflicts from scratch. We can use these results to look at the occurrence of topics over time. Thereby, we should bear in mind that the representation of topics depends on the composition of the underlying text collection.

Furthermore, we can use the topic model for thematic filtering of a text collection based on the topic probabilities for each document. By investigating these thematic text collections, we can use information retrieval approaches (e.g., the implicit network approach) to identify relations between these topics and other potential entities (e.g., state leaders, dates, and locations) and to dig deeper into the context (see @cujai_big_2023 for further details). 

<br>

### References

[^1]: The presented conflict trends are drawn from an adjusted version of the ICEWS data set which only contains interstate conflict actions. The data selection process reveals limited availability of data on interstate conflict events explaining the shortened investigation period. For more information on the selection process, see Cujai, S. (2021): Determination of Interstate Conflict Trends with ICEWS. https://tinyurl.com/2dyh67su